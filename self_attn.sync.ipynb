{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fc65869",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "Notes from Karpathy's lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81845f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ef37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch, time, channels\n",
    "B, T, C = 4, 8, 2\n",
    "x = torch.rand(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5357698",
   "metadata": {},
   "source": [
    "Simple averaging model (bag-of-words)\n",
    "Goal:\n",
    "$x[b,t] = \\text{mean}_{\\,i\\, \\le\\, t} (x[b,i])$\n",
    "\n",
    "- up to 8 tokens in a batch\n",
    "- want them to \"talk to each other\"\n",
    "  - but only with tokens that came before them\n",
    "  - information flows from previous context to the current timestep\n",
    "  - no information from the future, trying to predict it\n",
    "- simplest way for a token to \"communicate\" with other tokens is to take the mean of all previous tokens\n",
    "  - creates a feature vector that \"summarizes\" the current token in the context of the previous tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35090acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow = torch.zeros((B, T, C))  # bow = bag-of-words\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, : t + 1]  # (t, C)\n",
    "        xbow[b, t] = xprev.mean(dim=0)  # averages over time\n",
    "\n",
    "# batch 0\n",
    "print(x[0])\n",
    "# NOTE: first row is the same since there is no previous context to average over\n",
    "print(xbow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6197101",
   "metadata": {},
   "source": [
    "Implementation above is very inefficient $O(n^2)$\n",
    "The \"mathematical trick\" is to use matrix multiplication\n",
    "\n",
    "- multiplying by a lower triangular ones matrix will sum all previous tokens\n",
    "- if instead of using ones, we use a uniform probability distribution (all rows sum to 1)\n",
    "      we get an average of all previous tokens\n",
    "  - weighted aggregation, where the weights are equal in the lower triangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969b486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying by a lower triangular ones matrix\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "print(f\"a=\\n{a}\")\n",
    "print(f\"b=\\n{b}\")\n",
    "print(f\"a⋅b=\\n{a @ b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece9b279",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying by a lower triangular uniform probability distribution\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a = a / a.sum(dim=1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "print(f\"a=\\n{a}\")\n",
    "print(f\"b=\\n{b}\")\n",
    "print(f\"a⋅b=\\n{a @ b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8345df3",
   "metadata": {},
   "source": [
    "Bag of words vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.tril(torch.ones(T, T))\n",
    "w = w / w.sum(dim=1, keepdim=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e675ef52",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbow2 = w @ x  # (T, T) ⋅ (B, T, C) -> (B, T, C) (pytorch adds the batch dimension)\n",
    "xbow2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6e965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix multiplication essentially the same as the for loop above\n",
    "xbow2.allclose(xbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216ff31",
   "metadata": {},
   "source": [
    "Third implementation using softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8349f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "w = torch.zeros((T, T))\n",
    "\n",
    "# 1->0, 0->-inf\n",
    "w = w.masked_fill(tril == 0, float(\"-inf\"))\n",
    "\n",
    "# exponentiate and divide by sum\n",
    "# effectively normalizes so the row sums to 1 (gives the same as above)\n",
    "w = F.softmax(w, dim=-1)\n",
    "\n",
    "xbow3 = w @ x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac43edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb3b3ee",
   "metadata": {},
   "source": [
    "- When training self attention, weights start off as 0, giving the uniform distribution\n",
    "- As the model trains, the weights will change to reflect the importance of each previous token to the current token\n",
    "- Giving a weighted average instead of a uniform average\n",
    "- So, we use softmax to allow this training and get the weighted probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa37cc98",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "## Self-Attention Head\n",
    "\n",
    "- builds on the ideas above\n",
    "  - lower triangular weight matrix + softmax = weighted average of previous tokens\n",
    "  - i.e. gives an affinity score to each previous token for the current token\n",
    "- want to modify the zero weights to reflect the importance of each previous token\n",
    "  - data dependent token associations\n",
    "\n",
    "\n",
    "\n",
    "**Self attention solves this by having every node (every token at every timestep) emit 2 vectors:**\n",
    "1. **(K)ey - what do I contain?**\n",
    "2. **(Q)uery - what am I looking for?**\n",
    "\n",
    "**Then, to get the affinity between tokens, take the dot products between the current token's query and every other token's key**\n",
    "\n",
    "**This becomes the weights for the weighted average of the previous tokens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e4738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32  # 4×8 tokens with 32 channels of information per token\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "k = key(x)  # (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "\n",
    "# only transpose the last two dimensions, leave batch dimension\n",
    "w = q @ k.transpose(-2, -1)  # (B, T, head_size) ⋅ (B, head_size, T) -> (B, T, T)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "w = w.masked_fill(tril == 0, float(\"-inf\"))\n",
    "w = F.softmax(w, dim=-1)\n",
    "\n",
    "# aggregate the inputs before getting the output\n",
    "# x is \"information private to the current token\"\n",
    "# v is that information aggregated\n",
    "v = value(x)  # (B, T, head_size)\n",
    "out = w @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1ba4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "w[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020f1338",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "## Notes on attention\n",
    "\n",
    "1. Attention is a **communication mechanism** between tokens\n",
    "- tokens in a block can be thought of as nodes in a directed graph\n",
    "- each node contains a vector of info\n",
    "- can aggregate info via a weighted sum from all nodes that point to it \n",
    "    - (in a data dependent manner)\n",
    "- a token's node is pointed to by all previous tokens in the block at time T + itself\n",
    "\n",
    "2. There is no notion of (geometric) space\n",
    "- attention acts on a set of vectors\n",
    "- the nodes have no inherent position\n",
    "- this is why we need positional encoding\n",
    "\n",
    "3. Batches are completely independent\n",
    "- no information persists between batches\n",
    "- each batch is a completely independent graph\n",
    "\n",
    "4. **Decoder vs Encoder blocks**\n",
    "- only _decoder_ attention blocks prevent the current token from \"communicating\" with future tokens\n",
    "- the triangular masking makes this a decoder attention block\n",
    "- deleting that line allows all tokens to communicate with each other\n",
    "    - i.e. an _encoder_ attention block\n",
    "\n",
    "5. **Self-attention vs Cross-attention**\n",
    "- self-attention means the keys, queries, and values all come from the same source\n",
    "- can be generalized to cross attention, where the queries still come from $x$, but the keys and values come from a different source\n",
    "    - other source may be encoder blocks encoding some context we want to condition on\n",
    "\n",
    "6. **Scaled attention**\n",
    "- divides weights $w$ by $\\sqrt{\\text{head_size}}$ \n",
    "- when $Q,K$ are unit variance, $w$ will have unit variance too\n",
    "    - without scaling, $w$ will have variance ~head_size\n",
    "- this keeps softmax diffuse and prevents saturation (some values way overpowering others)\n",
    "    - saturated softmax approaches one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed3429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaled attention illustration\n",
    "\n",
    "# no scaling\n",
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "w = q @ k.transpose(-2, -1)\n",
    "print(\"no scaling\")\n",
    "print(f\"k.var())={k.var():.3f}\")\n",
    "print(f\"q.var())={q.var():.3f}\")\n",
    "print(f\"w.var())={w.var():.3f}\")\n",
    "\n",
    "# scaling\n",
    "w_scaled = w * (head_size**-0.5)  # ≡ w / head_size^2\n",
    "print(\"\\nscaled\")\n",
    "print(f\"w_scaled.var())={w_scaled.var():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8b4e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saturated softmax illustration\n",
    "\n",
    "vals = torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])\n",
    "diffuse = torch.softmax(vals, dim=-1)\n",
    "saturated = torch.softmax(vals * 8, dim=-1)\n",
    "\n",
    "print(f\"diffuse={diffuse}\")\n",
    "print(f\"saturated={saturated}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
